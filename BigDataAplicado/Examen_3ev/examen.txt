Coger archivos (json,csv,txt) y subirlo a s3 (usando sparkS3.py)

Nos da el script de creacion de sql, neo4j y/o mongo 
y tenemos que hacer una consulta de coger los datos que el pida y subirlo a s3

Nos da el producer y tenemos que hacer el consumer y subir los datos a S3 

Hacer un group (por dias u horas) y ordenarlo por semanas (por ejemplo)
Limpiar nulls 
Limpiar error (como queramos, borrarlo, usar media..)

Da igual si junto o por separado 

Tendriamos que haber generado 3 tablas, nos da el formato de las tablas (postgres) a traves de las consultas

Subir luego eso a la db en postgres desde s3 (es con un write)

Postgres ser√° un contenedor aparte 

Luego hacer queries con eso por pantalla

usar de ejemplo el read_database.py para las queries (si falla subir a s3 lo guardamos en archivos y subirlo a s3)




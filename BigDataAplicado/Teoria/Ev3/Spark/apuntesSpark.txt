-----------------------
LOCALSTACK
-----------------------
# Si hay un SparkSession no se ejcuta
docker exec -it localstack /bin/bash

-- Crear bucket 
awslocal s3api create-bucket --bucket sample-bucket 

-- Ver buckets 
awslocal s3api list-buckets 

-- Crear objeto 
awslocal s3api put-object --bucket sample-bucket --key image.jpg --body image.jpg

-- Ver contenido 
aws configure set aws_access_key_id test
aws configure set aws_secret_access_key test
aws configure set region us-east-1


aws --endpoint-url=http://localhost:4566 s3 ls s3://bucket/output/


-- Como conectarse a S3:

dentro de sparkS3.py 
Nos faltar√° una ip: 4566
Endpoint: http://localstack:4566
Si quiero ejecutar un job, poner el key id y el access key de aws 
El sdk y la dependencia a file system


Como podemos comprobar si un nombre de dominio funciona: hacer ping

-- Para subir archivo a S3 
Mas abajo meter en el read (en el try) de s3 poner un .csv que exista dentro de apps

Si no es un .csv se cambiaria el final (.csv) al formato que es el archivo


-----------------------
POSTGRES
-----------------------

docker exec -it postgres-db /bin/bash 

psql -U postgres -d retail_db -c 'SELECT * FROM Stores limit 10'

#Nos metemos a sparkmaster
python read_database.py 
(posible error, cambiar url de jdbc a postgres_db)






-----------------------
KAFKA
-----------------------
# Ejecutamos para q produzca
python kafka_producer.py
# Ejecutando esto lo veriamos
python kafka_consumer.py 

# Para generar datos en kafka, hace falta unos datos (json (?)) y el schema
# Ejecutamos el streamingkafka desde el master (desde apps)
python streamingkafka.py

# Si vamos a localstack y entramos dentro del bucket veremos el /sales









{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Google Colab\n"],"metadata":{"id":"uaXwBRKMdG84"}},{"cell_type":"markdown","source":["Google Colab es una herramienta online gratuita basada en la nube que permite desplegar modelos de aprendizaje automático de forma remota en **CPUs y GPUs**\n","\n","Se basa en la tecnología de código abierto **Jupyter**, con la que se puede crear un cuaderno **Ipython**. El cuaderno Ipython no sólo te ofrece la posibilidad de escribir código, sino también de *contar una historia* a través de él.\n","\n","Puedes ejecutar código, crear visualizaciones dedatos, y escribir sobre cada paso que se haga en texto plano o markdown. Esto hace que el código incluya explicaciones y visualizaciones de lo que hace."],"metadata":{"id":"xnxonCkcdVxc"}},{"cell_type":"markdown","source":["¿Que recursos ofrece Colab?\n"],"metadata":{"id":"vT19_2audX6m"}},{"cell_type":"markdown","source":["- Uso de Disco"],"metadata":{"id":"dI94YgptdglK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MnpJQcGOdA6l","outputId":"8d198435-611b-4b0f-c0df-44f73d7d1125","executionInfo":{"status":"ok","timestamp":1677257239369,"user_tz":-60,"elapsed":13,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Filesystem      Size  Used Avail Use% Mounted on\n","overlay         108G   33G   76G  31% /\n","tmpfs            64M     0   64M   0% /dev\n","shm             5.8G     0  5.8G   0% /dev/shm\n","/dev/root       2.0G  1.1G  841M  58% /usr/sbin/docker-init\n","tmpfs           6.4G   36K  6.4G   1% /var/colab\n","/dev/sda1        70G   43G   27G  62% /etc/hosts\n","tmpfs           6.4G     0  6.4G   0% /proc/acpi\n","tmpfs           6.4G     0  6.4G   0% /proc/scsi\n","tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"]}],"source":["!df -h"]},{"cell_type":"markdown","source":["- CPU's"],"metadata":{"id":"0TdEAryWdiD5"}},{"cell_type":"code","source":["!cat /proc/cpuinfo | grep \"model name\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKG7LPFVdfCG","outputId":"572fc401-cb4d-4a65-a58b-bd38296f19df","executionInfo":{"status":"ok","timestamp":1677257239369,"user_tz":-60,"elapsed":6,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model name\t: AMD EPYC 7B12\n","model name\t: AMD EPYC 7B12\n"]}]},{"cell_type":"markdown","source":["* Memoria"],"metadata":{"id":"v8JsiHaGdkv1"}},{"cell_type":"code","source":["!cat /proc/meminfo | grep \"Mem\"*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4ieBk40dkc3","outputId":"c601a355-8591-4120-fe5e-c9378be135c9","executionInfo":{"status":"ok","timestamp":1677257239683,"user_tz":-60,"elapsed":318,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MemTotal:       13297192 kB\n","MemFree:         8540096 kB\n","MemAvailable:   12485348 kB\n"]}]},{"cell_type":"markdown","source":["* GPU\n","\n","Si activamos el soporte GPU entonces podremos obtener información sobre la misma\n","Información sobre la GPU disponible\n","\n","`nvidia-smi -L` solo lista el nombre\n","\n","`nvidia-smi -q ` mucha más información disponible"],"metadata":{"id":"RRhzD1mJdrlo"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_dzxrA6duc3","outputId":"fbbd201f-93e4-4edd-e70b-ce11064eb8e9","executionInfo":{"status":"ok","timestamp":1677258657005,"user_tz":-60,"elapsed":210,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"id":"imTz6nmwdve-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677258659367,"user_tz":-60,"elapsed":263,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}},"outputId":"fdf00d73-e4b2-4dba-914b-44950ebcdac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Tue_Mar__8_18:18:20_PST_2022\n","Cuda compilation tools, release 11.6, V11.6.124\n","Build cuda_11.6.r11.6/compiler.31057947_0\n"]}]},{"cell_type":"code","source":["!cat /proc/driver/nvidia/gpus/0000:00:04.0/information"],"metadata":{"id":"8izVXeDedw7d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677258679029,"user_tz":-60,"elapsed":6,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}},"outputId":"2e331827-d317-4b78-d918-4c17bb6c3eea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cat: /proc/driver/nvidia/gpus: No such file or directory\n"]}]},{"cell_type":"markdown","source":["# Hadoop"],"metadata":{"id":"d3-j-3-td0sZ"}},{"cell_type":"markdown","source":["Hadoop es un marco de programación basado en Java que permite procesar y almacenar conjuntos de datos extremadamente grandes en un clúster de máquinas de bajo coste. Fue el primer gran proyecto de código abierto en el ámbito del Big Data y está patrocinado por la Apache Software Foundation."],"metadata":{"id":"HBwwvw6GeHys"}},{"cell_type":"markdown","source":["## Instalación de Hadoop"],"metadata":{"id":"zvMUeqI8eKzd"}},{"cell_type":"markdown","source":["* bajar la distribución correspondiente"],"metadata":{"id":"N45XMNJfeNBK"}},{"cell_type":"code","source":["!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"],"metadata":{"id":"FaSWVHUBeKJp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677257596773,"user_tz":-60,"elapsed":30487,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}},"outputId":"b0072a19-0288-4242-ade9-6f18ea1a5eca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-24 16:52:46--  https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n","Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f9:3a:2c57::2, ...\n","Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 695457782 (663M) [application/x-gzip]\n","Saving to: ‘hadoop-3.3.4.tar.gz’\n","\n","hadoop-3.3.4.tar.gz 100%[===================>] 663.24M  23.3MB/s    in 30s     \n","\n","2023-02-24 16:53:16 (22.5 MB/s) - ‘hadoop-3.3.4.tar.gz’ saved [695457782/695457782]\n","\n"]}]},{"cell_type":"markdown","source":["* extraerla en el sistema de archivos de colab"],"metadata":{"id":"RvoEmhuXeP58"}},{"cell_type":"code","source":["!tar -xzf hadoop-3.3.6.tar.gz"],"metadata":{"id":"TZuSfPveesx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* mover la distribución de hadoop  /usr/local"],"metadata":{"id":"K0pqGNYve2Sc"}},{"cell_type":"code","source":["!mv  hadoop-3.3.6/ /usr/local/"],"metadata":{"id":"DYpR8ywaeq7F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configuración"],"metadata":{"id":"GTC-UUBGekB7"}},{"cell_type":"markdown","source":["* actualizamos variables de entorno (JAVA_HOME, PATH)"],"metadata":{"id":"FN2a-M_GfFXb"}},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/usr/local/hadoop-3.3.6/bin\""],"metadata":{"id":"JGalbE25fEy_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* comprobamos instalación"],"metadata":{"id":"AH5ZYJOWfMIs"}},{"cell_type":"code","source":["!hadoop version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1Zqjd5DfOfP","outputId":"18907e0b-6de6-42d6-c911-4e78be9f69f5","executionInfo":{"status":"ok","timestamp":1677257609330,"user_tz":-60,"elapsed":1113,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hadoop 3.3.4\n","Source code repository https://github.com/apache/hadoop.git -r a585a73c3e02ac62350c136643a5e7f6095a3dbb\n","Compiled by stevel on 2022-07-29T12:32Z\n","Compiled with protoc 3.7.1\n","From source with checksum fb9dd8918a7b8a5b430d61af858f6ec\n","This command was run using /usr/local/hadoop-3.3.4/share/hadoop/common/hadoop-common-3.3.4.jar\n"]}]},{"cell_type":"markdown","source":["## Ejecución de ejemplos"],"metadata":{"id":"QghmXYaEfmXd"}},{"cell_type":"markdown","source":["Una de las formas tradicionales de asegurarnos que un ambiente de Hadoop recién instalado funciona correctamente, es ejecutando el *jar* de ejemplos *map-reduce* incluido con toda instalación de hadoop (*hadoop-mapreduce-examples.jar*).\n","\n","[Hadoop Map Reduce Examples](http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/)"],"metadata":{"id":"LDz8lgGGfwHj"}},{"cell_type":"markdown","source":["1. Creamos un directorio de ficheros en los que volquemos los xml de hadoop"],"metadata":{"id":"qG7QK3i5fxqk"}},{"cell_type":"code","source":["%%bash\n","mkdir ~/input\n","cp /usr/local/hadoop-3.3.6/etc/hadoop/*.xml ~/input\n","ls ~/input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4XDprG74fowi","outputId":"61bd367c-7af7-4799-9546-886d62b4aaf6","executionInfo":{"status":"ok","timestamp":1677257609331,"user_tz":-60,"elapsed":5,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["capacity-scheduler.xml\n","core-site.xml\n","hadoop-policy.xml\n","hdfs-rbf-site.xml\n","hdfs-site.xml\n","httpfs-site.xml\n","kms-acls.xml\n","kms-site.xml\n","mapred-site.xml\n","yarn-site.xml\n"]}]},{"cell_type":"markdown","source":["2. Ejecutamos hadoop jar con el fin de ejecutar uno de los ejemplos por defecto, en este caso el grep que busca expresiones regulares dentro de los ficheros que le especifiquemos."],"metadata":{"id":"FNMVFo_Df4f_"}},{"cell_type":"code","source":["%%bash\n","hadoop jar \\\n","  /usr/local/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar \\\n","  grep ~/input ~/grep_example 'allowed[.]*'"],"metadata":{"id":"iffiuEsZgJf9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677257614590,"user_tz":-60,"elapsed":5262,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}},"outputId":"0b2bc152-3cd7-4fa3-ed27-1a23d97642c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2023-02-24 16:53:30,919 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2023-02-24 16:53:31,126 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2023-02-24 16:53:31,126 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2023-02-24 16:53:31,600 INFO input.FileInputFormat: Total input files to process : 10\n","2023-02-24 16:53:31,625 INFO mapreduce.JobSubmitter: number of splits:10\n","2023-02-24 16:53:31,945 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1093092602_0001\n","2023-02-24 16:53:31,945 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2023-02-24 16:53:32,146 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2023-02-24 16:53:32,147 INFO mapreduce.Job: Running job: job_local1093092602_0001\n","2023-02-24 16:53:32,158 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2023-02-24 16:53:32,166 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,167 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,167 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n","2023-02-24 16:53:32,217 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2023-02-24 16:53:32,218 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000000_0\n","2023-02-24 16:53:32,242 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,242 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,267 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,272 INFO mapred.MapTask: Processing split: file:/root/input/hadoop-policy.xml:0+11765\n","2023-02-24 16:53:32,313 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,313 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,313 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,313 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,313 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,318 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,345 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,345 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,345 INFO mapred.MapTask: Spilling map output\n","2023-02-24 16:53:32,345 INFO mapred.MapTask: bufstart = 0; bufend = 374; bufvoid = 104857600\n","2023-02-24 16:53:32,345 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214312(104857248); length = 85/6553600\n","2023-02-24 16:53:32,357 INFO mapred.MapTask: Finished spill 0\n","2023-02-24 16:53:32,366 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000000_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,370 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,370 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000000_0' done.\n","2023-02-24 16:53:32,376 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000000_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=293934\n","\t\tFILE: Number of bytes written=920525\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=275\n","\t\tMap output records=22\n","\t\tMap output bytes=374\n","\t\tMap output materialized bytes=25\n","\t\tInput split bytes=99\n","\t\tCombine input records=22\n","\t\tCombine output records=1\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=311427072\n","\tFile Input Format Counters \n","\t\tBytes Read=11765\n","2023-02-24 16:53:32,376 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000000_0\n","2023-02-24 16:53:32,376 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000001_0\n","2023-02-24 16:53:32,379 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,379 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,380 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,381 INFO mapred.MapTask: Processing split: file:/root/input/capacity-scheduler.xml:0+9213\n","2023-02-24 16:53:32,422 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,423 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,423 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,423 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,423 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,437 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,452 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,453 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,453 INFO mapred.MapTask: Spilling map output\n","2023-02-24 16:53:32,453 INFO mapred.MapTask: bufstart = 0; bufend = 16; bufvoid = 104857600\n","2023-02-24 16:53:32,453 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2023-02-24 16:53:32,454 INFO mapred.MapTask: Finished spill 0\n","2023-02-24 16:53:32,461 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000001_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,463 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,463 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000001_0' done.\n","2023-02-24 16:53:32,464 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000001_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=304139\n","\t\tFILE: Number of bytes written=920581\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=244\n","\t\tMap output records=1\n","\t\tMap output bytes=16\n","\t\tMap output materialized bytes=24\n","\t\tInput split bytes=104\n","\t\tCombine input records=1\n","\t\tCombine output records=1\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=19\n","\t\tTotal committed heap usage (bytes)=311427072\n","\tFile Input Format Counters \n","\t\tBytes Read=9213\n","2023-02-24 16:53:32,464 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000001_0\n","2023-02-24 16:53:32,464 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000002_0\n","2023-02-24 16:53:32,469 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,470 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,470 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,471 INFO mapred.MapTask: Processing split: file:/root/input/kms-acls.xml:0+3518\n","2023-02-24 16:53:32,505 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,512 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,512 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,512 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,512 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,515 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,520 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,520 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,523 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000002_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,527 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,528 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000002_0' done.\n","2023-02-24 16:53:32,528 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000002_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=308649\n","\t\tFILE: Number of bytes written=920619\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=135\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=94\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=7\n","\t\tTotal committed heap usage (bytes)=311427072\n","\tFile Input Format Counters \n","\t\tBytes Read=3518\n","2023-02-24 16:53:32,528 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000002_0\n","2023-02-24 16:53:32,528 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000003_0\n","2023-02-24 16:53:32,530 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,530 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,531 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,532 INFO mapred.MapTask: Processing split: file:/root/input/hdfs-site.xml:0+775\n","2023-02-24 16:53:32,557 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,557 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,557 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,557 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,557 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,558 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,565 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,566 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,574 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000003_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,575 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,575 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000003_0' done.\n","2023-02-24 16:53:32,576 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000003_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=310416\n","\t\tFILE: Number of bytes written=920657\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=21\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=95\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=8\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=775\n","2023-02-24 16:53:32,576 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000003_0\n","2023-02-24 16:53:32,576 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000004_0\n","2023-02-24 16:53:32,580 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,581 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,582 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,585 INFO mapred.MapTask: Processing split: file:/root/input/core-site.xml:0+774\n","2023-02-24 16:53:32,608 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,614 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,614 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,614 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,614 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,629 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,633 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,633 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,635 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000004_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,637 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,637 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000004_0' done.\n","2023-02-24 16:53:32,637 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000004_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=312182\n","\t\tFILE: Number of bytes written=920695\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=20\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=95\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=774\n","2023-02-24 16:53:32,638 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000004_0\n","2023-02-24 16:53:32,638 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000005_0\n","2023-02-24 16:53:32,641 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,641 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,642 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,643 INFO mapred.MapTask: Processing split: file:/root/input/mapred-site.xml:0+758\n","2023-02-24 16:53:32,659 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,659 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,659 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,659 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,659 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,660 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,666 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,666 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,668 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000005_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,670 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,670 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000005_0' done.\n","2023-02-24 16:53:32,671 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000005_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=313932\n","\t\tFILE: Number of bytes written=920733\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=21\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=97\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=758\n","2023-02-24 16:53:32,672 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000005_0\n","2023-02-24 16:53:32,672 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000006_0\n","2023-02-24 16:53:32,677 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,677 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,678 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,688 INFO mapred.MapTask: Processing split: file:/root/input/yarn-site.xml:0+690\n","2023-02-24 16:53:32,712 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,713 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,713 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,713 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,713 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,715 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,717 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,717 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,721 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000006_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,723 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,724 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000006_0' done.\n","2023-02-24 16:53:32,725 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000006_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=315102\n","\t\tFILE: Number of bytes written=920771\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=19\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=95\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=690\n","2023-02-24 16:53:32,725 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000006_0\n","2023-02-24 16:53:32,725 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000007_0\n","2023-02-24 16:53:32,727 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,727 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,728 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,728 INFO mapred.MapTask: Processing split: file:/root/input/hdfs-rbf-site.xml:0+683\n","2023-02-24 16:53:32,741 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,741 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,741 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,741 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,741 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,742 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,745 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,745 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,747 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000007_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,755 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,755 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000007_0' done.\n","2023-02-24 16:53:32,756 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000007_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=316265\n","\t\tFILE: Number of bytes written=920809\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=20\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=683\n","2023-02-24 16:53:32,756 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000007_0\n","2023-02-24 16:53:32,756 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000008_0\n","2023-02-24 16:53:32,761 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,761 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,762 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,771 INFO mapred.MapTask: Processing split: file:/root/input/kms-site.xml:0+682\n","2023-02-24 16:53:32,782 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,782 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,782 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,782 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,782 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,785 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,789 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,789 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,793 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000008_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,795 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,795 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000008_0' done.\n","2023-02-24 16:53:32,796 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000008_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=317427\n","\t\tFILE: Number of bytes written=920847\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=20\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=94\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=682\n","2023-02-24 16:53:32,796 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000008_0\n","2023-02-24 16:53:32,796 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_m_000009_0\n","2023-02-24 16:53:32,797 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,797 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,798 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,799 INFO mapred.MapTask: Processing split: file:/root/input/httpfs-site.xml:0+620\n","2023-02-24 16:53:32,814 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:32,814 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:32,814 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:32,814 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:32,814 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:32,815 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:32,818 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:32,818 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:32,837 INFO mapred.Task: Task:attempt_local1093092602_0001_m_000009_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,839 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:32,839 INFO mapred.Task: Task 'attempt_local1093092602_0001_m_000009_0' done.\n","2023-02-24 16:53:32,839 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_m_000009_0: Counters: 18\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=318527\n","\t\tFILE: Number of bytes written=920885\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=17\n","\t\tMap output records=0\n","\t\tMap output bytes=0\n","\t\tMap output materialized bytes=6\n","\t\tInput split bytes=97\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=620\n","2023-02-24 16:53:32,839 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_m_000009_0\n","2023-02-24 16:53:32,839 INFO mapred.LocalJobRunner: map task executor complete.\n","2023-02-24 16:53:32,842 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2023-02-24 16:53:32,843 INFO mapred.LocalJobRunner: Starting task: attempt_local1093092602_0001_r_000000_0\n","2023-02-24 16:53:32,853 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:32,853 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:32,854 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:32,855 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a8840dc\n","2023-02-24 16:53:32,857 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2023-02-24 16:53:32,876 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2023-02-24 16:53:32,883 INFO reduce.EventFetcher: attempt_local1093092602_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2023-02-24 16:53:32,913 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000006_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,916 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000006_0\n","2023-02-24 16:53:32,920 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n","2023-02-24 16:53:32,925 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000009_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,926 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000009_0\n","2023-02-24 16:53:32,926 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->4\n","2023-02-24 16:53:32,927 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,930 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000002_0\n","2023-02-24 16:53:32,930 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 4, usedMemory ->6\n","2023-02-24 16:53:32,931 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000005_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,932 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000005_0\n","2023-02-24 16:53:32,932 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 4, commitMemory -> 6, usedMemory ->8\n","2023-02-24 16:53:32,933 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000008_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,934 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000008_0\n","2023-02-24 16:53:32,934 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 5, commitMemory -> 8, usedMemory ->10\n","2023-02-24 16:53:32,935 WARN io.ReadaheadPool: Failed readahead on ifile\n","EBADF: Bad file descriptor\n","\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n","\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:419)\n","\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:296)\n","\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:220)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","2023-02-24 16:53:32,935 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000001_0 decomp: 20 len: 24 to MEMORY\n","2023-02-24 16:53:32,936 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local1093092602_0001_m_000001_0\n","2023-02-24 16:53:32,936 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 6, commitMemory -> 10, usedMemory ->30\n","2023-02-24 16:53:32,937 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000004_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,937 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000004_0\n","2023-02-24 16:53:32,938 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 7, commitMemory -> 30, usedMemory ->32\n","2023-02-24 16:53:32,939 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000007_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,939 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000007_0\n","2023-02-24 16:53:32,939 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 8, commitMemory -> 32, usedMemory ->34\n","2023-02-24 16:53:32,940 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000000_0 decomp: 21 len: 25 to MEMORY\n","2023-02-24 16:53:32,940 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output for attempt_local1093092602_0001_m_000000_0\n","2023-02-24 16:53:32,940 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 21, inMemoryMapOutputs.size() -> 9, commitMemory -> 34, usedMemory ->55\n","2023-02-24 16:53:32,941 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1093092602_0001_m_000003_0 decomp: 2 len: 6 to MEMORY\n","2023-02-24 16:53:32,942 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1093092602_0001_m_000003_0\n","2023-02-24 16:53:32,942 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 10, commitMemory -> 55, usedMemory ->57\n","2023-02-24 16:53:32,944 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2023-02-24 16:53:32,945 INFO mapred.LocalJobRunner: 10 / 10 copied.\n","2023-02-24 16:53:32,945 INFO reduce.MergeManagerImpl: finalMerge called with 10 in-memory map-outputs and 0 on-disk map-outputs\n","2023-02-24 16:53:32,950 INFO mapred.Merger: Merging 10 sorted segments\n","2023-02-24 16:53:32,951 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 20 bytes\n","2023-02-24 16:53:32,952 INFO reduce.MergeManagerImpl: Merged 10 segments, 57 bytes to disk to satisfy reduce memory limit\n","2023-02-24 16:53:32,952 INFO reduce.MergeManagerImpl: Merging 1 files, 43 bytes from disk\n","2023-02-24 16:53:32,953 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2023-02-24 16:53:32,953 INFO mapred.Merger: Merging 1 sorted segments\n","2023-02-24 16:53:32,953 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n","2023-02-24 16:53:32,954 INFO mapred.LocalJobRunner: 10 / 10 copied.\n","2023-02-24 16:53:32,979 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2023-02-24 16:53:32,980 INFO mapred.Task: Task:attempt_local1093092602_0001_r_000000_0 is done. And is in the process of committing\n","2023-02-24 16:53:32,986 INFO mapred.LocalJobRunner: 10 / 10 copied.\n","2023-02-24 16:53:32,989 INFO mapred.Task: Task attempt_local1093092602_0001_r_000000_0 is allowed to commit now\n","2023-02-24 16:53:32,990 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1093092602_0001_r_000000_0' to file:/content/grep-temp-2030755944\n","2023-02-24 16:53:32,994 INFO mapred.LocalJobRunner: reduce > reduce\n","2023-02-24 16:53:32,994 INFO mapred.Task: Task 'attempt_local1093092602_0001_r_000000_0' done.\n","2023-02-24 16:53:32,994 INFO mapred.Task: Final Counters for attempt_local1093092602_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=318987\n","\t\tFILE: Number of bytes written=921075\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=2\n","\t\tReduce shuffle bytes=97\n","\t\tReduce input records=2\n","\t\tReduce output records=2\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =10\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=10\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=147\n","2023-02-24 16:53:32,994 INFO mapred.LocalJobRunner: Finishing task: attempt_local1093092602_0001_r_000000_0\n","2023-02-24 16:53:32,994 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2023-02-24 16:53:33,152 INFO mapreduce.Job: Job job_local1093092602_0001 running in uber mode : false\n","2023-02-24 16:53:33,153 INFO mapreduce.Job:  map 100% reduce 100%\n","2023-02-24 16:53:33,154 INFO mapreduce.Job: Job job_local1093092602_0001 completed successfully\n","2023-02-24 16:53:33,178 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=3429560\n","\t\tFILE: Number of bytes written=10128197\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=792\n","\t\tMap output records=23\n","\t\tMap output bytes=390\n","\t\tMap output materialized bytes=97\n","\t\tInput split bytes=969\n","\t\tCombine input records=23\n","\t\tCombine output records=2\n","\t\tReduce input groups=2\n","\t\tReduce shuffle bytes=97\n","\t\tReduce input records=2\n","\t\tReduce output records=2\n","\t\tSpilled Records=4\n","\t\tShuffled Maps =10\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=10\n","\t\tGC time elapsed (ms)=51\n","\t\tTotal committed heap usage (bytes)=4189061120\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29478\n","\tFile Output Format Counters \n","\t\tBytes Written=147\n","2023-02-24 16:53:33,204 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2023-02-24 16:53:33,218 INFO input.FileInputFormat: Total input files to process : 1\n","2023-02-24 16:53:33,220 INFO mapreduce.JobSubmitter: number of splits:1\n","2023-02-24 16:53:33,242 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local137539743_0002\n","2023-02-24 16:53:33,242 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2023-02-24 16:53:33,340 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2023-02-24 16:53:33,340 INFO mapreduce.Job: Running job: job_local137539743_0002\n","2023-02-24 16:53:33,341 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2023-02-24 16:53:33,341 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:33,341 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:33,342 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n","2023-02-24 16:53:33,346 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2023-02-24 16:53:33,347 INFO mapred.LocalJobRunner: Starting task: attempt_local137539743_0002_m_000000_0\n","2023-02-24 16:53:33,349 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:33,349 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:33,350 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:33,353 INFO mapred.MapTask: Processing split: file:/content/grep-temp-2030755944/part-r-00000:0+135\n","2023-02-24 16:53:33,390 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2023-02-24 16:53:33,390 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2023-02-24 16:53:33,390 INFO mapred.MapTask: soft limit at 83886080\n","2023-02-24 16:53:33,390 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2023-02-24 16:53:33,390 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2023-02-24 16:53:33,395 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2023-02-24 16:53:33,404 INFO mapred.LocalJobRunner: \n","2023-02-24 16:53:33,404 INFO mapred.MapTask: Starting flush of map output\n","2023-02-24 16:53:33,404 INFO mapred.MapTask: Spilling map output\n","2023-02-24 16:53:33,404 INFO mapred.MapTask: bufstart = 0; bufend = 33; bufvoid = 104857600\n","2023-02-24 16:53:33,404 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n","2023-02-24 16:53:33,406 INFO mapred.MapTask: Finished spill 0\n","2023-02-24 16:53:33,409 INFO mapred.Task: Task:attempt_local137539743_0002_m_000000_0 is done. And is in the process of committing\n","2023-02-24 16:53:33,410 INFO mapred.LocalJobRunner: map\n","2023-02-24 16:53:33,410 INFO mapred.Task: Task 'attempt_local137539743_0002_m_000000_0' done.\n","2023-02-24 16:53:33,411 INFO mapred.Task: Final Counters for attempt_local137539743_0002_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=600291\n","\t\tFILE: Number of bytes written=1836231\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=2\n","\t\tMap output records=2\n","\t\tMap output bytes=33\n","\t\tMap output materialized bytes=43\n","\t\tInput split bytes=112\n","\t\tCombine input records=0\n","\t\tSpilled Records=2\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tFile Input Format Counters \n","\t\tBytes Read=147\n","2023-02-24 16:53:33,411 INFO mapred.LocalJobRunner: Finishing task: attempt_local137539743_0002_m_000000_0\n","2023-02-24 16:53:33,411 INFO mapred.LocalJobRunner: map task executor complete.\n","2023-02-24 16:53:33,412 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2023-02-24 16:53:33,412 INFO mapred.LocalJobRunner: Starting task: attempt_local137539743_0002_r_000000_0\n","2023-02-24 16:53:33,415 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2023-02-24 16:53:33,415 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2023-02-24 16:53:33,415 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2023-02-24 16:53:33,415 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3eced8ba\n","2023-02-24 16:53:33,416 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2023-02-24 16:53:33,417 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2023-02-24 16:53:33,418 INFO reduce.EventFetcher: attempt_local137539743_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2023-02-24 16:53:33,423 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local137539743_0002_m_000000_0 decomp: 39 len: 43 to MEMORY\n","2023-02-24 16:53:33,423 INFO reduce.InMemoryMapOutput: Read 39 bytes from map-output for attempt_local137539743_0002_m_000000_0\n","2023-02-24 16:53:33,423 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 39, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->39\n","2023-02-24 16:53:33,424 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2023-02-24 16:53:33,424 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2023-02-24 16:53:33,424 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2023-02-24 16:53:33,425 INFO mapred.Merger: Merging 1 sorted segments\n","2023-02-24 16:53:33,426 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n","2023-02-24 16:53:33,429 INFO reduce.MergeManagerImpl: Merged 1 segments, 39 bytes to disk to satisfy reduce memory limit\n","2023-02-24 16:53:33,429 INFO reduce.MergeManagerImpl: Merging 1 files, 43 bytes from disk\n","2023-02-24 16:53:33,429 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2023-02-24 16:53:33,429 INFO mapred.Merger: Merging 1 sorted segments\n","2023-02-24 16:53:33,429 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n","2023-02-24 16:53:33,430 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2023-02-24 16:53:33,433 INFO mapred.Task: Task:attempt_local137539743_0002_r_000000_0 is done. And is in the process of committing\n","2023-02-24 16:53:33,435 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2023-02-24 16:53:33,435 INFO mapred.Task: Task attempt_local137539743_0002_r_000000_0 is allowed to commit now\n","2023-02-24 16:53:33,436 INFO output.FileOutputCommitter: Saved output of task 'attempt_local137539743_0002_r_000000_0' to file:/root/grep_example\n","2023-02-24 16:53:33,437 INFO mapred.LocalJobRunner: reduce > reduce\n","2023-02-24 16:53:33,437 INFO mapred.Task: Task 'attempt_local137539743_0002_r_000000_0' done.\n","2023-02-24 16:53:33,437 INFO mapred.Task: Final Counters for attempt_local137539743_0002_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=600409\n","\t\tFILE: Number of bytes written=1836308\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=2\n","\t\tReduce shuffle bytes=43\n","\t\tReduce input records=2\n","\t\tReduce output records=2\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=406847488\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=34\n","2023-02-24 16:53:33,437 INFO mapred.LocalJobRunner: Finishing task: attempt_local137539743_0002_r_000000_0\n","2023-02-24 16:53:33,437 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2023-02-24 16:53:34,341 INFO mapreduce.Job: Job job_local137539743_0002 running in uber mode : false\n","2023-02-24 16:53:34,341 INFO mapreduce.Job:  map 100% reduce 100%\n","2023-02-24 16:53:34,341 INFO mapreduce.Job: Job job_local137539743_0002 completed successfully\n","2023-02-24 16:53:34,344 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1200700\n","\t\tFILE: Number of bytes written=3672539\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=2\n","\t\tMap output records=2\n","\t\tMap output bytes=33\n","\t\tMap output materialized bytes=43\n","\t\tInput split bytes=112\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=2\n","\t\tReduce shuffle bytes=43\n","\t\tReduce input records=2\n","\t\tReduce output records=2\n","\t\tSpilled Records=4\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=813694976\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=147\n","\tFile Output Format Counters \n","\t\tBytes Written=34\n"]}]},{"cell_type":"code","source":["!cat ~/grep_example/*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkteR8esgZTs","outputId":"e8bd0f65-d0f8-467e-d5ec-9f0aae667e85","executionInfo":{"status":"ok","timestamp":1677257614591,"user_tz":-60,"elapsed":15,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["22\tallowed.\n","1\tallowed\n"]}]},{"cell_type":"markdown","source":["# HDFS\n"],"metadata":{"id":"2aHq4zezd-09"}},{"cell_type":"markdown","source":["Las siguientes sentencias únicamente sirven para probar comandos básicos de HDFS no para gestionar una Infraestructura que en Google Colab no existe, en este caso el sistema de archivos HDFS es el mismo que el local"],"metadata":{"id":"KCkyAP77gkGi"}},{"cell_type":"markdown","metadata":{"id":"dPB6hnMc_Xeh"},"source":["* Crear el directorio *prueba*"]},{"cell_type":"code","metadata":{"id":"HAGdEAYy_UDB"},"source":["!hdfs dfs -mkdir prueba"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qt4ovs9-vC47"},"source":["- Crear un fichero local :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOCZWC2JvUOH","outputId":"5406ba33-6ca2-4651-fc64-d895254a4c0b","executionInfo":{"status":"ok","timestamp":1677257620946,"user_tz":-60,"elapsed":4,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["%%bash\n","echo \"Ejemplo de HDFS\" > user.txt\n","echo `date` >> user.txt\n","cat user.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ejemplo de HDFS\n","Fri 24 Feb 2023 04:53:40 PM UTC\n"]}]},{"cell_type":"code","metadata":{"id":"Pk4NIb_gvdNg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"58df2dff-dd96-4cbf-be4d-1ce374952997","executionInfo":{"status":"ok","timestamp":1677257623685,"user_tz":-60,"elapsed":2742,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["!hdfs dfs -put user.txt prueba/\n","!hdfs dfs -ls prueba"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1 items\n","-rw-r--r--   1 root root         48 2023-02-24 16:53 prueba/user.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"dXxnK8JA_0_9"},"source":["- Mostrar su contenido"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3C_cq5OvxNV","outputId":"b47f6b18-5d57-4deb-fc31-9d813565c04e","executionInfo":{"status":"ok","timestamp":1677257624901,"user_tz":-60,"elapsed":1219,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["!hdfs dfs -cat prueba/user.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ejemplo de HDFS\n","Fri 24 Feb 2023 04:53:40 PM UTC\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRKP6oHOHcNL","outputId":"cbbb99b9-72a6-4fb8-e0e8-6aa807100348","executionInfo":{"status":"ok","timestamp":1677257626457,"user_tz":-60,"elapsed":1558,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["%%bash\n","hdfs dfs -tail prueba/user.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ejemplo de HDFS\n","Fri 24 Feb 2023 04:53:40 PM UTC\n"]}]},{"cell_type":"markdown","source":["# SPARK"],"metadata":{"id":"_Giekut_eBDD"}},{"cell_type":"markdown","source":["- [Apache Spark](https://spark.apache.org) se lanzó por primera vez en 2014.\n","- Fue desarrollado originalmente por [Matei Zaharia](http://people.csail.mit.edu/matei) como un proyecto de clase, y más tarde una tesis doctoral, en la Universidad de California, Berkeley.\n","- Spark está escrito en [Scala](https://www.scala-lang.org).\n","- Todas las imágenes proceden de [Databricks](https://databricks.com/product/getting-started-guide).\n","- Apache Spark es un sistema de computación en clúster rápido y de propósito general.\n","- Proporciona API de alto nivel en Java, Scala, Python y R, y un motor optimizado que soporta gráficos de ejecución general.\n","- Spark puede gestionar colecciones de \"big data\" con un pequeño conjunto de primitivas de alto nivel como `map`, `filter`, `groupby` y `join`.  Con estos patrones comunes a menudo podemos manejar cálculos que son más complejos que map, pero siguen siendo estructurados.\n","- También es compatible con un amplio conjunto de herramientas de alto nivel, como [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) para SQL y el procesamiento de datos estructurados, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) para el aprendizaje automático, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) para el procesamiento de gráficos y Spark Streaming."],"metadata":{"id":"wQwmLEcbHPnu"}},{"cell_type":"markdown","source":["## Ciclo de vida de un programa Spark\n","\n","1. Crea algunos RDD de entrada a partir de datos externos o paraleliza una colección en tu programa controlador.\n","2. Transformarlos perezosamente para definir nuevos RDDs usando transformaciones como `filter()` o `map()`.\n","3. Pedir a Spark que almacene en caché() cualquier RDD intermedio que deba ser reutilizado.\n","4. Lanzar acciones como count() y collect() para iniciar un cálculo paralelo, que luego es optimizado y ejecutado por Spark.\n"],"metadata":{"id":"XhhfuEl6Hung"}},{"cell_type":"markdown","source":["## Operaciones con datos distribuidos\n","\n","- Dos tipos de operaciones: **transformaciones** y **acciones**.\n","- Las transformaciones son *lazy* (no se calculan inmediatamente)\n","- Las transformaciones se ejecutan cuando se ejecuta una acción"],"metadata":{"id":"VZS9-7EnIOnK"}},{"cell_type":"markdown","source":["## [Transformaciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n","\n","```\n","map() flatMap()\n","filter()\n","mapPartitions() mapPartitionsWithIndex()\n","sample()\n","union() intersection() distinct()\n","groupBy() groupByKey()\n","reduceBy() reduceByKey()\n","sortBy() sortByKey()\n","join()\n","cogroup()\n","cartesian()\n","pipe()\n","coalesce()\n","repartition()\n","partitionBy()\n","...\n","```"],"metadata":{"id":"SigIRrQAIWep"}},{"cell_type":"markdown","source":["## [Acciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n","\n","```\n","reduce()\n","collect()\n","count()\n","first()\n","take()\n","takeSample()\n","saveToCassandra()\n","takeOrdered()\n","saveAsTextFile()\n","saveAsSequenceFile()\n","saveAsObjectFile()\n","countByKey()\n","foreach()\n","```"],"metadata":{"id":"nqZvEFpGIgMs"}},{"cell_type":"markdown","source":["## Python API\n","\n","PySpark utiliza Py4J, que permite a los programas Python acceder dinámicamente a objetos Java.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1Llin_Zd-11YhHrRds6l_M3GAJWU3x2Ly)\n"],"metadata":{"id":"x7VEPoqgInjt"}},{"cell_type":"markdown","source":["## La clase SparkContext\n","\n","- Cuando trabajamos con Apache Spark invocamos métodos sobre un objeto que es una instancia del contexto `pyspark.SparkContext`.\n","\n","- Típicamente, una instancia de este objeto se crea automáticamente y se asigna a la variable `sc`.\n","\n","- El método `parallelize` de `SparkContext` se puede utilizar para convertir cualquier colección ordinaria de Python en un RDD;\n","    - normalmente crearíamos un RDD a partir de un archivo grande o una tabla HBase."],"metadata":{"id":"tdVKAOB3KxIs"}},{"cell_type":"markdown","metadata":{"id":"2I7p_uqUiIb3"},"source":["## Instalación"]},{"cell_type":"markdown","metadata":{"id":"ri7MiwC8fOqi"},"source":["En primer lugar instalamos y configuramos todas las dependencias de Spark para Python. De esta forma enlazaremos nuestro entorno con el servidor de Spark. Además configuraremos el entorno Spark con las variables que sean necesarias.\n","\n","\n","\n","**NOTA:**\n","\n","1. **la última versión de PySpark es la 3.3.2 [link](https://pypi.org/project/pyspark/#history)**\n","\n","2. **Puede tardar un poco tiempo en hacer todos los procesos de descarga de datos pyspark tardar en descargar en su entorno virtual**\n"]},{"cell_type":"markdown","source":["* Descargamos y comprimimos la instalación de Spark\n"],"metadata":{"id":"JJ93-T-7SLhh"}},{"cell_type":"code","metadata":{"id":"XWM4ntmGfLSE"},"source":["# Install spark-related dependencies\n","!wget -q http://apache.osuosl.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n","!tar xf spark-3.3.2-bin-hadoop3.tgz\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q findspark\n","!pip install pyspark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNwYkLIBSJuK","outputId":"2da30316-818d-4530-ef76-efb63870e3fa","executionInfo":{"status":"ok","timestamp":1677257849697,"user_tz":-60,"elapsed":6534,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.2)\n","Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"]}]},{"cell_type":"markdown","source":["* Configurar las variables de entorno"],"metadata":{"id":"UtXmFZKXjZJY"}},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""],"metadata":{"id":"dVArSVXhjVN-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dq4j8GfmSRY"},"source":["Vamos a verificar que el entorno Spark está bien creado"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"4pMTURnTgoIe","outputId":"44bd3e9f-6c74-4e37-93cc-abdbeb305582","executionInfo":{"status":"ok","timestamp":1677257890871,"user_tz":-60,"elapsed":4,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["os.environ[\"SPARK_HOME\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/spark-3.3.2-bin-hadoop3'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"CCyZxBpShBiP"},"source":["Vamos a iniciar uan sesión de spark simple para testear nuestra instalación\n","\n","1. Ejecutamos findspark.init() para hacer que pyspark sea importable como una biblioteca normal"]},{"cell_type":"code","metadata":{"id":"Y7nrYBCBhBOh"},"source":["import findspark\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LZqWpiYnpGt"},"source":["2. Crear un contexto Spark para ejecutar la aplicación\n","\n","> NOTA: Un SparkContext representa la conexión al cluster de Spark, y puede utilizarse para crear RDDs y otros elementos. **Sólo puede haber un SparkContext activo**. Se debe detener (*stop()*) el SparkContext activo antes de crear uno nuevo."]},{"cell_type":"code","metadata":{"id":"8E2ccfuQWsug"},"source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMbWlE-0nbJl","colab":{"base_uri":"https://localhost:8080/","height":196},"outputId":"c5a314ed-8194-4451-d78e-a9d1a681ef98","executionInfo":{"status":"ok","timestamp":1677257905171,"user_tz":-60,"elapsed":5380,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["conf = SparkConf()\n","conf.set(\"spark.ui.port\", \"4050\")\n","conf.set(\"spark.appName\", \"Pi\")\n","# create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()\n","sc\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SparkContext master=local[*] appName=pyspark-shell>"],"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://266ae51ef8ea:4050\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"QeZ3vsFxpMmT"},"source":["3. Plantear el problema a resolver. En este caso la aproximación de Pi por el método de Montecarlo [enlace](https://www.geogebra.org/m/cF7RwK3H)\n"]},{"cell_type":"code","metadata":{"id":"1lcoGXBzkU5s"},"source":["import random\n","num_samples = 1000000\n","def inside(p):\n","  x, y = random.random(), random.random()\n","  return x*x + y*y < 1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRxOanjhpCoJ"},"source":["4. Paralelizar el cálculo con **parallelize**: Este método se utiliza para distribuir la colección de elementos del mismo tipo (datos u operaciones) para poder funcionar en paralelo."]},{"cell_type":"code","metadata":{"id":"JzoJEvPtoUNB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4e5c1bb-8250-4cf6-dc91-2da3226d3e70","executionInfo":{"status":"ok","timestamp":1677257920864,"user_tz":-60,"elapsed":3258,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n","count\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["784985"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"po4pk0sJprwq"},"source":["5. Obtenemos el resultado final"]},{"cell_type":"code","metadata":{"id":"672qv1phpq8g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5838c89-c014-4da4-c2b7-f5a776e69308","executionInfo":{"status":"ok","timestamp":1677257920864,"user_tz":-60,"elapsed":5,"user":{"displayName":"Rafael Alamañac","userId":"05366170063085049833"}}},"source":["pi = 4 * count / num_samples\n","print(pi)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.13994\n"]}]},{"cell_type":"markdown","metadata":{"id":"DBUPnpdIoWRu"},"source":["6. Paramos el SparkContext"]},{"cell_type":"code","metadata":{"id":"kB9mF_zJoVal"},"source":["sc.stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P4oTq-kgGphT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spark/Hive"],"metadata":{"id":"oG8LaVPd69Gn"}},{"cell_type":"markdown","metadata":{"id":"0hzFRvMz-iHU"},"source":["## Spark-Hive"]},{"cell_type":"markdown","metadata":{"id":"sBfZGn3YA_TK"},"source":["1. Instalación"]},{"cell_type":"code","metadata":{"id":"nqld9bk_-mF7"},"source":["!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n","!mv  spark-3.2.1-bin-hadoop3.2 /usr/local/\n","!pip install -q findspark\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/usr/local/spark-3.2.1-bin-hadoop3.2\"\n"],"metadata":{"id":"Md8rfmP87Wlz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import findspark\n","findspark.init()"],"metadata":{"id":"aw0xbBn37f3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ARyob50A8CR"},"source":["2. Crear Contexto pyspark"]},{"cell_type":"code","source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf"],"metadata":{"id":"D09Rj23C7oeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"wHFAANWX_hHe","executionInfo":{"status":"ok","timestamp":1648226128160,"user_tz":-60,"elapsed":6317,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"4fe5e80d-3728-4740-f8c2-1f826a71e8f3"},"source":["conf = SparkConf()\n","conf.set(\"spark.ui.port\", \"4050\")\n","conf.set(\"spark.appName\", \"SQL\")\n","#without test\n","conf.set(\"spark.executor.instances\", \"4\")\n","conf.set(\"spark.executor.cores\", 4)\n","conf.set(\"spark.executor.memory\",\"10g\")\n","\n","# create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()\n","sc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SparkContext master=local[*] appName=pyspark-shell>"],"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://edc28dc267eb:4050\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.2.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"2Am597Zjymcp"},"source":["3. Monitorización de Spark"]},{"cell_type":"markdown","source":["Como no se puede utilizar un teórico localhost sobre Google Colab entonces hay que hacer accesible el puerto desde el exterior mediante un tunel ngrok\n","\n","NGROK es una herramienta de uso gratuito que nos permite exponer nuestro entorno local a la web, es decir, podemos \"publicar\" nuestro trabajo en local para que el resto del mundo lo pueda ver sin la necesidad de subir la aplicación a un servidor."],"metadata":{"id":"VEBUhbrg9afw"}},{"cell_type":"markdown","source":["**Subir fichero authtoken.txt con la clave de acceso que proporciona ngrok al registrarse.**"],"metadata":{"id":"6qyJXuz_8spW"}},{"cell_type":"code","source":["!pip install pyngrok\n","!ngrok authtoken $(<authtoken.txt)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8gy8jdx8ebA","executionInfo":{"status":"ok","timestamp":1648226549675,"user_tz":-60,"elapsed":5624,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"d6450e4c-19e6-43f4-d3ad-b3a6d8f8f64b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 24.8 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=ae727f674db2c943bdb2335a7d908c1348e133ffeb3b8993356c81d18dae417d\n","  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.1.0\n","Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","public_url = ngrok.connect(4050)\n","public_url"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Avv4q6WF8lUV","executionInfo":{"status":"ok","timestamp":1648226555582,"user_tz":-60,"elapsed":623,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"c293722c-0ad6-4151-fa59-e63b22907d71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["t=2022-03-25T16:42:36+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"]},{"output_type":"execute_result","data":{"text/plain":["<NgrokTunnel: \"http://6609-35-245-128-92.ngrok.io\" -> \"http://localhost:4050\">"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"PfqSLQlJyoLx"},"source":["4. Spark para lectura de JSON"]},{"cell_type":"code","source":["%%bash\n","wget https://ias1.larioja.org/opendata/download?r=Y2Q9ODYxfGNmPTA0\n","mv /content/download?r=Y2Q9ODYxfGNmPTA0 /content/pruebas_localidades.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdXQpSbz-RX4","executionInfo":{"status":"ok","timestamp":1648227092091,"user_tz":-60,"elapsed":2398,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"013de8e1-a290-4ed5-cf63-54d278b17f08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["--2022-03-25 16:51:30--  https://ias1.larioja.org/opendata/download?r=Y2Q9ODYxfGNmPTA0\n","Resolving ias1.larioja.org (ias1.larioja.org)... 195.55.164.30\n","Connecting to ias1.larioja.org (ias1.larioja.org)|195.55.164.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/json]\n","Saving to: ‘download?r=Y2Q9ODYxfGNmPTA0’\n","\n","     0K .......... .......... .......... .......... ......      205K=0.2s\n","\n","2022-03-25 16:51:33 (205 KB/s) - ‘download?r=Y2Q9ODYxfGNmPTA0’ saved [48001]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KBigFfgE1Z5","executionInfo":{"status":"ok","timestamp":1648228923753,"user_tz":-60,"elapsed":493,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"4dd401d4-bbad-4f7b-8529-0ef48e18ecf7"},"source":["df = spark.read.json(\"/content/datos.json\")\n","df.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- data: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- CATEGORIA: string (nullable = true)\n"," |    |    |-- CENTRO_TRABAJO: string (nullable = true)\n"," |    |    |-- DIVISION: string (nullable = true)\n"," |    |    |-- NUMERO: double (nullable = true)\n"," |    |    |-- TIPO_PERSONAL: string (nullable = true)\n"," |    |    |-- UNIDAD_PLAZA: string (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n","df2 = df.select(explode(\"data\")).select(col(\"col.*\"))\n","df2.printSchema()\n","df2.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vduWGRafDMIw","executionInfo":{"status":"ok","timestamp":1648229550039,"user_tz":-60,"elapsed":588,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"49e99282-8042-4fe9-f036-83dc4d5b0c84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- CATEGORIA: string (nullable = true)\n"," |-- CENTRO_TRABAJO: string (nullable = true)\n"," |-- DIVISION: string (nullable = true)\n"," |-- NUMERO: double (nullable = true)\n"," |-- TIPO_PERSONAL: string (nullable = true)\n"," |-- UNIDAD_PLAZA: string (nullable = true)\n","\n","+--------------------+--------------------+--------------------+------+--------------------+--------------------+\n","|           CATEGORIA|      CENTRO_TRABAJO|            DIVISION|NUMERO|       TIPO_PERSONAL|        UNIDAD_PLAZA|\n","+--------------------+--------------------+--------------------+------+--------------------+--------------------+\n","|        DIRECTOR/A 1| SERVICIOS CENTRALES|           DIRECCION|   1.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|                FEAS| SERVICIOS CENTRALES|           DIRECCION|   2.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|GRUPO DE GESTION ...| SERVICIOS CENTRALES|           DIRECCION|   1.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|MEDICO DE FAMILIA...| SERVICIOS CENTRALES|           DIRECCION|   2.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|                null| SERVICIOS CENTRALES|           DIRECCION|   6.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|     A.T.S. / D.U.E.|   C.SALUD DE NAJERA|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|                null|   C.SALUD DE NAJERA|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|     A.T.S. / D.U.E.|   C.SALUD ESPARTERO|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|     A.T.S. / D.U.E.|           ESPARTERO|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|                null|                null|PERSONAL EN FORMA...|   2.0|ENFERMERO/A INTER...|DEPARTAMENTO DE S...|\n","|TRABAJOS AUXILIAR...|UNIDAD  DE APOYO ...|    OTRAS DIVISIONES|   4.0|ESTADO DE ALARMA ...|UNIDAD DE APOYO A...|\n","|  MEDICO  DE FAMILIA|CENTRO ASISTENCIA...|              MEDICA|   1.0|         FACULTATIVO|CENTRO ASISTENCIA...|\n","|                null|CENTRO ASISTENCIA...|              MEDICA|   1.0|         FACULTATIVO|CENTRO ASISTENCIA...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   1.0|         FACULTATIVO|      ADMINISTRACIÓN|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   7.0|         FACULTATIVO|CENTRO COORDINADO...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   5.0|         FACULTATIVO|CENTRO COORDINADO...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   2.0|         FACULTATIVO|UNIDAD MOVIL DE E...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   5.0|         FACULTATIVO|UNIDAD MOVIL DE E...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   6.0|         FACULTATIVO|UNIDAD MOVIL DE E...|\n","|MEDICO DE FAMILIA...|    C.SALUD CASCAJOS|              MEDICA|   1.0|         FACULTATIVO|UNIDAD DE APOYO A...|\n","+--------------------+--------------------+--------------------+------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["MapPartitionsRDD[213] at toJavaRDD at NativeMethodAccessorImpl.java:0"]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"B3dVIL8Syqta"},"source":["4. Spark Hive para datos Json"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUHekGp-_siT","executionInfo":{"status":"ok","timestamp":1648230178692,"user_tz":-60,"elapsed":603,"user":{"displayName":"curso-ia-bd uclm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09458479208673268791"}},"outputId":"a25dc3ce-192c-4d87-e08c-52320d680541"},"source":["from pyspark.sql import HiveContext\n","hiveCtx = spark.builder.enableHiveSupport().getOrCreate()\n","ex1 = hiveCtx.read.json(df2.toJSON())\n","ex1.registerTempTable(\"datos\")\n","results = hiveCtx.sql(\"SELECT * FROM datos\").show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n","  FutureWarning\n"]},{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+--------------------+------+--------------------+--------------------+\n","|           CATEGORIA|      CENTRO_TRABAJO|            DIVISION|NUMERO|       TIPO_PERSONAL|        UNIDAD_PLAZA|\n","+--------------------+--------------------+--------------------+------+--------------------+--------------------+\n","|        DIRECTOR/A 1| SERVICIOS CENTRALES|           DIRECCION|   1.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|                FEAS| SERVICIOS CENTRALES|           DIRECCION|   2.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|GRUPO DE GESTION ...| SERVICIOS CENTRALES|           DIRECCION|   1.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|MEDICO DE FAMILIA...| SERVICIOS CENTRALES|           DIRECCION|   2.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|                null| SERVICIOS CENTRALES|           DIRECCION|   6.0|          DIRECTIVOS|DIRECCIÓN ÁREA DE...|\n","|     A.T.S. / D.U.E.|   C.SALUD DE NAJERA|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|                null|   C.SALUD DE NAJERA|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|     A.T.S. / D.U.E.|   C.SALUD ESPARTERO|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|     A.T.S. / D.U.E.|           ESPARTERO|PERSONAL EN FORMA...|   1.0|ENFERMERO/A INTER...|ZONA BASICA DE SA...|\n","|                null|                null|PERSONAL EN FORMA...|   2.0|ENFERMERO/A INTER...|DEPARTAMENTO DE S...|\n","|TRABAJOS AUXILIAR...|UNIDAD  DE APOYO ...|    OTRAS DIVISIONES|   4.0|ESTADO DE ALARMA ...|UNIDAD DE APOYO A...|\n","|  MEDICO  DE FAMILIA|CENTRO ASISTENCIA...|              MEDICA|   1.0|         FACULTATIVO|CENTRO ASISTENCIA...|\n","|                null|CENTRO ASISTENCIA...|              MEDICA|   1.0|         FACULTATIVO|CENTRO ASISTENCIA...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   1.0|         FACULTATIVO|      ADMINISTRACIÓN|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   7.0|         FACULTATIVO|CENTRO COORDINADO...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   5.0|         FACULTATIVO|CENTRO COORDINADO...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   2.0|         FACULTATIVO|UNIDAD MOVIL DE E...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   5.0|         FACULTATIVO|UNIDAD MOVIL DE E...|\n","|MEDICO DE EMERGEN...|CENTRO COORDINADO...|              MEDICA|   6.0|         FACULTATIVO|UNIDAD MOVIL DE E...|\n","|MEDICO DE FAMILIA...|    C.SALUD CASCAJOS|              MEDICA|   1.0|         FACULTATIVO|UNIDAD DE APOYO A...|\n","+--------------------+--------------------+--------------------+------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}]}]}